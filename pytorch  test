import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import torch
from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader

# 设定输入输出
a=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
b=[1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0]

x=np.zeros((10,1))
y=np.zeros((10,1))

for i in range(len(x)):
    x[i,0]=a[i]
    y[i,0]=(0.5+2*a[i])/3  #输入归一化以适应神经网络
plt.plot(x,y)
train_set=(x,y)
#在pytorch里数据以tensor的形式存在，在进行传播求解时还要把tensor转换为variable。
train_set=torch.tensor(train_set, dtype=torch.float32) #pytorch拟合需要目标为32浮点

for i in range(len(x)):
    x[i,0]=b[i]
    y[i,0]=(0.5+2*b[i])/3  #输入归一化以适应神经网络
test_set=(x,y)    
test_set=torch.tensor(test_set, dtype=torch.float32)  #pytorch拟合需要目标为32浮点

train_data = DataLoader(train_set, batch_size=len(x), shuffle=True)
test_data = DataLoader(test_set, batch_size=len(x), shuffle=False)

#pytorch linear regression
# 设定网络
net = nn.Sequential(
    nn.Linear(1, 1)  #如是1-1的网络，则等价于1维的线性回归
#     nn.Linear(1, 2),
#     nn.ReLU(),
#     nn.Linear(2, 2),
#     nn.ReLU(),
#     nn.Linear(2, 1)
    )

# 开始训练
criterion = nn.MSELoss()  #以均方误差评估模型
optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)  #以梯度下降法优化神经网络参数，学习率0.01，动量）

losses = []
eval_losses = []

for e in range(20):
    train_loss = 0
    net.train()  # 将模型改为训练模式
    for x, y in train_data:
        x = Variable(x)
        y = Variable(y)
        # 前向传播
        out = net(x)
        loss = criterion(out, y)
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # 记录误差
        train_loss += loss.item()        
    losses.append(train_loss)

    # 在测试集上检验效果
    eval_loss = 0
    net.eval() # 将模型改为预测模式
    for x, y in test_data:
        x = Variable(x)
        y = Variable(y)
        out = net(x)
        loss = criterion(out, y)
        # 记录误差
        eval_loss += loss.item()        
    eval_losses.append(eval_loss)
    
eval_out=[]
eval_origin=[]
for i in range(len(x)):
    extract_origin=y.data[i].numpy() # 把值从tensor转为numpy array再提取值
    eval_origin.append(extract_origin[0])
    
    extract_predict=out.data[i].numpy() # 把值从tensor转为numpy array再提取值
    eval_out.append(extract_predict[0])

#    print('epoch: {}, Train Loss: {:.6f}, Eval Loss: {:.6f}'.format(e, train_loss , eval_loss))

fig=plt.figure()
ax1=fig.add_subplot(211) #2*1的图形 在第一个位置
ax1.set_title('loss and fit')
ax1.plot(np.arange(len(losses)), losses, label='train loss')
ax1.plot(np.arange(len(eval_losses)), eval_losses, label='test loss')
ax1.legend()

ax2=fig.add_subplot(212) #2*1的图形 在第一个位置
ax2.plot(eval_origin, label='oringin Y')
ax2.plot(eval_out, label='predict Y')
ax2.legend()

#pytorch NN
# 设定网络
net = nn.Sequential(
     nn.Linear(1, 2),
     nn.ReLU(),
     nn.Linear(2, 1)
    )

criterion = nn.MSELoss()  #以均方误差评估模型
optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)  #以梯度下降法优化神经网络参数，学习率0.01，动量减少局部最优解可能）

# 开始训练
losses = []
eval_losses = []

for e in range(20):
    train_loss = 0
    net.train()  # 将模型改为训练模式
    for x, y in train_data:
        x = Variable(x)
        y = Variable(y)
        # 前向传播
        out = net(x)
        loss = criterion(out, y)
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # 记录误差
        train_loss += loss.item()        
    losses.append(train_loss)

    # 在测试集上检验效果
    eval_loss = 0
    net.eval() # 将模型改为预测模式
    for x, y in test_data:
        x = Variable(x)
        y = Variable(y)
        out = net(x)
        loss = criterion(out, y)
        # 记录误差
        eval_loss += loss.item()        
    eval_losses.append(eval_loss)
    
eval_out=[]
eval_origin=[]
for i in range(len(x)):
    extract_origin=y.data[i].numpy() # 把值从tensor转为numpy array再提取值
    eval_origin.append(extract_origin[0])
    
    extract_predict=out.data[i].numpy() # 把值从tensor转为numpy array再提取值
    eval_out.append(extract_predict[0])

#    print('epoch: {}, Train Loss: {:.6f}, Eval Loss: {:.6f}'.format(e, train_loss , eval_loss))

fig=plt.figure()
ax1=fig.add_subplot(211) #2*1的图形 在第一个位置
ax1.set_title('loss and fit')
ax1.plot(np.arange(len(losses)), losses, label='train loss')
ax1.plot(np.arange(len(eval_losses)), eval_losses, label='test loss')
ax1.legend()

ax2=fig.add_subplot(212) #2*1的图形 在第一个位置
ax2.plot(eval_origin, label='oringin Y')
ax2.plot(eval_out, label='predict Y')
ax2.legend()

#ae
# 定义网络
class autoencoder(nn.Module):
    def __init__(self):
        super(autoencoder, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(1, 2),
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(2, 1),
        )

    def forward(self, x):
        encode = self.encoder(x)
        decode = self.decoder(encode)
        return encode, decode

net = autoencoder()

criterion = nn.MSELoss()  #以均方误差评估模型
optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)  #以梯度下降法优化神经网络参数，学习率0.01，动量减少局部最优解可能）

# 开始训练自动编码器
losses = []
eval_losses = []

for e in range(20):
    train_loss = 0
    net.train()  # 将模型改为训练模式
    for x, y in train_data:
        x = Variable(x)
        y = Variable(y)
        # 前向传播
        code, out = net(x)
        loss = criterion(out, x)
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # 记录误差
        train_loss += loss.item()        
    losses.append(train_loss)
    
        # 在测试集上检验效果
    eval_loss = 0
    net.eval() # 将模型改为预测模式
    for x, y in test_data:
        x = Variable(x)
        y = Variable(y)
        code, out = net(x)
        loss = criterion(out, x)
        # 记录误差
        eval_loss += loss.item()        
    eval_losses.append(eval_loss)
    
eval_out=[]
eval_origin=[]
for i in range(len(x)):
    extract_origin=y.data[i].numpy() # 把值从tensor转为numpy array再提取值
    eval_origin.append(extract_origin[0])
    
    extract_predict=out.data[i].numpy() # 把值从tensor转为numpy array再提取值
    eval_out.append(extract_predict[0])

#    print('epoch: {}, Train Loss: {:.6f}, Eval Loss: {:.6f}'.format(e, train_loss , eval_loss))

fig=plt.figure()
ax1=fig.add_subplot(211) #2*1的图形 在第一个位置
ax1.set_title('loss and fit')
ax1.plot(np.arange(len(losses)), losses, label='train loss')
ax1.plot(np.arange(len(eval_losses)), eval_losses, label='test loss')
ax1.legend()

ax2=fig.add_subplot(212) #2*1的图形 在第一个位置
ax2.plot(eval_origin, label='oringin X')
ax2.plot(eval_out, label='predict X')
ax2.legend()    

#code plot
ae_code0=[]
ae_code1=[]
label_origin=[]
label_out=[]
for i in range(len(x)):
    
    extract_code0=code.data[i,0].numpy().tolist() # 把值从tensor转为numpy array再python list提取值    
    ae_code0.append(extract_code0)
    
    extract_code1=code.data[i,1].numpy().tolist() # 把值从tensor转为numpy array再python list提取值  
    ae_code1.append(extract_code1)    
    
    label_origin.append(0)
    label_out.append(1)

Xa = ae_code0 + ae_code0
Ya = ae_code1 + ae_code1
Za = eval_origin + eval_out   
label=label_origin + label_out  

from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(2)
ax = Axes3D(fig)    # 3D 图
ax.set_xlim(min(Xa),max(Xa))
ax.set_ylim(min(Ya),max(Ya))
ax.set_zlim(min(Za), max(Za))
for x, y, z, s in zip(Xa, Ya, Za, label):
    c = cm.rainbow(int(255*s))    # 上色
    ax.text(x, y, z, s, backgroundcolor=c)  # 标位子

